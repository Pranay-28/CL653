# -*- coding: utf-8 -*-
"""210107063_Project_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/127WLqM8K93wSulN1Mm2Nrm7bVWE6nKAH
"""

!wget https://raw.githubusercontent.com/Pranay-28/CL653/main/winequality-red.csv

import warnings
warnings.filterwarnings("ignore")
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
!pip install optuna

df = pd.read_csv('/content/winequality-red.csv', skiprows=[41])

df.head(8)

df.columns = [col.replace(" ", "_") for col in df.columns]

df.info()

df.isnull().sum()

df.nunique()

df.duplicated().sum()

df.drop_duplicates(inplace = True)

df = df.reset_index(drop=True)

num_cols_custom = [col_custom for col_custom in df.columns if (df[col_custom].dtype in ["int64","float64"]) & (df[col_custom].nunique()>50)]
num_cols_custom

target_custom = [col_custom for col_custom in df.columns if df[col_custom].nunique()<10]
target_custom

fig_custom, ax_custom = plt.subplots(1,2,figsize=(10, 3), width_ratios=[2,1])

textprops_custom={'fontsize': 12, 'weight': 'bold',"color": "black"}
ax_custom[0].pie(df["quality"].value_counts().to_list(),
        colors=["#abc9ea","#98daa7","#f3aba8","#d3c3f7","#f3f3af","#c0ebe9"],
        labels=df["quality"].value_counts().index.to_list(),
        autopct='%1.f%%',
        explode=([.05]*3 +[.9,.9,.9]),
        pctdistance=0.5,
        wedgeprops={'linewidth' : 1, 'edgecolor' : 'black'},
        textprops=textprops_custom)

sns.countplot(x = "quality", data=df, palette = "pastel6", order=df["quality"].value_counts().to_dict().keys())
for p, count in enumerate(df["quality"].value_counts().to_dict().values(),0):
    ax_custom[1].text(p-0.2, count + 8, count, color='black', fontsize=12)
plt.setp(ax_custom[1].get_xticklabels(), fontweight="bold")
plt.yticks([])
plt.box(False)
fig_custom.suptitle(x=0.56, t=f'► Quality Distribution ◄', fontsize=18, fontweight='bold')
plt.tight_layout()
plt.show()

plt.figure(figsize=(10,len(num_cols_custom)*2))
for idx_custom,column_custom in enumerate(num_cols_custom):
    plt.subplot(len(num_cols_custom)//2+1,2,idx_custom+1)
    sns.boxplot(x="quality", y=column_custom, data=df,palette="pastel")
    plt.title(f"{column_custom} Distribution")
    plt.tight_layout()

plt.figure(figsize=(10,len(num_cols_custom)*2))
for idx_custom,column_custom in enumerate(num_cols_custom):
    plt.subplot(len(num_cols_custom)//2+1,2,idx_custom+1)
    sns.histplot(x=column_custom, hue="quality", data=df,bins=30,kde=True, palette="pastel")
    plt.title(f"{column_custom} Distribution")
    plt.tight_layout()

plt.figure(figsize=(8,6))
corr_custom=df[num_cols_custom].corr(numeric_only=True)
mask_custom = np.triu(np.ones_like(corr_custom))
sns.heatmap(corr_custom, annot=True, fmt=".1f", linewidths=1, mask=mask_custom, cmap=sns.color_palette("icefire"));

def new_features(df):
    # 1. Acidity Ratios
    df['TotalAcidity'] = df['fixed_acidity'] + df['volatile_acidity'] + df['citric_acid']

    # 2. Free Sulfur Dioxide / Total Sulfur Dioxide Ratio
    df['FreeSulfurToTotalSulfurRatio'] = df['free_sulfur_dioxide'] / df['total_sulfur_dioxide']

    # 3. Alcohol and Sugar Ratio
    df['AlcoholToSugarRatio'] = df['alcohol'] / df['residual_sugar']

    # 4. Free Sulfur Dioxide / pH Ratio
    df['FreeSulfurToPHRatio'] = df['free_sulfur_dioxide'] / df['pH']

    # 5. Alcohol and Acidity Ratio
    df['AlcoholToAcidityRatio'] = df['alcohol'] / df['TotalAcidity']

    # 6. Density and Sugar Ratio
    df['DensityToSugarRatio'] = df['density'] / df['residual_sugar']

    # 7. Various Mathematical Operations
    df['AcidityMinusPH'] = df['TotalAcidity'] - df['pH']
    df['AlcoholTimesSulfates'] = df['alcohol'] * df['sulphates']

    # 8. Alcohol Classes
    df['AlcoholClass'] = pd.cut(df['alcohol'],
                                bins=[0, 10, 12, float('inf')],
                                labels= ['Low', 'Medium', 'High'],
                                include_lowest=True).map({"Low":0,"Medium":1,"High":2}).astype(int)

    # 9. pH and Acidity Ratio
    df['PHToAcidityRatio'] = df['pH'] / df['TotalAcidity']

    # 10. Chlorides and Density Ratio
    df['ChloridesToDensityRatio'] = df['chlorides'] / df['density']

    # 11. Alcohol and Sulphates Ratio
    df['AlcoholToSulphatesRatio'] = df['alcohol'] / df['sulphates']

new_features(df)

num_cols_custom = [col_custom for col_custom in df.columns if (df[col_custom].dtype in ["int64","float64"]) & (df[col_custom].nunique()>50)]
cat_cols_custom = [col_custom for col_custom in df.columns if df[col_custom].nunique()<6]
cat_cols_custom

df = pd.get_dummies(df,columns=cat_cols_custom, drop_first=True, dtype="int")

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X = df.drop(["quality"], axis=1)
y = df["quality"]
y = y - 3  # Classes were 3-4-5-6-7-8, changed them to 0-1-2-3-4-5 temporarily for modeling

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=63)

# Hyper-parameter tuning
import optuna
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Define objective function for Optuna
def objective(trial):
    params = {
        'objective': 'multiclass',
        'metric': 'multi_logloss',
        'verbosity': -1,
        'boosting_type': 'gbdt',
        'num_leaves': trial.suggest_int('num_leaves', 2, 256),
        'max_depth': trial.suggest_int('max_depth', 1, 32),
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),
        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1.0),
        'subsample': trial.suggest_uniform('subsample', 0.1, 1.0),
        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-9, 10.0),
        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-9, 10.0),
        'random_state': 42,
        'n_jobs': -1
    }

    lgbm = LGBMClassifier(**params)
    lgbm.fit(X_train, y_train)
    y_pred = lgbm.predict(X_test)
    return 1 - accuracy_score(y_test, y_pred)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=63)

# Create and run the Optuna study
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=20)

# Print the best parameters and best score
print('Best trial:')
trial = study.best_trial
print(f'  Value: {trial.value}')
print('  Params: ')
for key, value in trial.params.items():
    print(f'    {key}: {value}')

import lightgbm
lgb = lightgbm.LGBMClassifier(objective = "multiclass")
lgb.fit(X_train, y_train)
lightgbm.plot_importance(lgb);
accuracy_score(y_test,lgb.predict(X_test))

import xgboost
xgb = xgboost.XGBClassifier(objective = "multi:softmax")
xgb.fit(X_train, y_train)
xgboost.plot_importance(xgb);
accuracy_score(y_test,xgb.predict(X_test))

from sklearn.preprocessing import MinMaxScaler
lgb_importances_custom = pd.DataFrame(dict(lgbm = lgb.feature_importances_), index=lgb.feature_name_)
xgb_importances_custom = pd.DataFrame(dict(xgb = xgb.feature_importances_), index=xgb.feature_names_in_)
importances_custom = pd.concat([lgb_importances_custom, xgb_importances_custom], axis=1)
min_max = MinMaxScaler((1,1.1))
importances_custom["cross"] = min_max.fit_transform(importances_custom[["lgbm"]]) * min_max.fit_transform(importances_custom[["xgb"]])
sorted_custom = importances_custom.sort_values(by="cross", ascending=False)
sorted_custom

import matplotlib
matplotlib.rc_file_defaults()

from lightgbm import LGBMClassifier
import optuna

def objective_lgb(trial):
    """Define the objective function"""

    params = {
        'objective': trial.suggest_categorical('objective', ['multiclass']),
        'max_depth': trial.suggest_int('max_depth', 1, 10),
        'min_child_samples': trial.suggest_int('min_child_samples', 1, 15),
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.05),
        'n_estimators': trial.suggest_int('n_estimators', 300, 700),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'subsample': trial.suggest_float('subsample', 0.1, 0.9),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.01, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 1.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 1.0),
        "seed" : trial.suggest_categorical('seed', [42]),
    }


    model_lgb = LGBMClassifier(**params)
    model_lgb.fit(X_train, y_train)
    y_pred = model_lgb.predict(X_test)
    return accuracy_score(y_test,y_pred)

study_lgb = optuna.create_study(direction='maximize')
optuna.logging.set_verbosity(optuna.logging.WARNING)
study_lgb.optimize(objective_lgb, n_trials=50,show_progress_bar=True)

print('Best parameters', study_lgb.best_params)

lgb = LGBMClassifier(**study_lgb.best_params)
lgb.fit(X_train, y_train)
y_pred = lgb.predict(X_test)

print('Accuracy: ', accuracy_score(y_test, y_pred))

from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_estimator(xgb,X_test, y_test,cmap="RdPu");

from xgboost import XGBClassifier
import optuna
def objective_xg(trial):
    """Define the objective function"""

    params = {
        'booster': trial.suggest_categorical('booster', ['gbtree']),
        'max_depth': trial.suggest_int('max_depth', 1, 10),
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.05),
        'n_estimators': trial.suggest_int('n_estimators', 300, 1000),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),
        'subsample': trial.suggest_loguniform('subsample', 0.3, 0.9),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 1.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 1.0),
        "seed" : trial.suggest_categorical('seed', [42]),
        'objective': trial.suggest_categorical('objective', ['multi:softmax']),
    }
    model_xgb = XGBClassifier(**params)
    model_xgb.fit(X_train, y_train)
    y_pred = model_xgb.predict(X_test)
    return accuracy_score(y_test,y_pred)

study_xgb = optuna.create_study(direction='maximize')
optuna.logging.set_verbosity(optuna.logging.WARNING)
study_xgb.optimize(objective_xg, n_trials=50,show_progress_bar=True)

print('Best parameters', study_xgb.best_params)

xgb = XGBClassifier(**study_xgb.best_params)
xgb.fit(X_train, y_train)
y_pred = xgb.predict(X_test)

print('Accuracy: ', accuracy_score(y_test, y_pred))

from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_estimator(xgb,X_test, y_test,cmap="RdPu");

from sklearn.ensemble import VotingClassifier
voting = VotingClassifier(estimators=[
                                      ('lgbm', lgb),
                                      ('xgb', xgb)], voting='hard')
voting.fit(X_train,y_train)
voting_pred = voting.predict(X_test)

print('Accuracy: ', accuracy_score(y_test, voting_pred))

from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_estimator(voting,X_test, y_test,cmap="RdPu");